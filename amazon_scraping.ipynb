{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61824bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1512b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonLaptopScraper:\n",
    "    def __init__(self, pincode,city):\n",
    "        \"\"\"\n",
    "        Initialize the AmazonLaptopScraper class with pincode and city.\n",
    "\n",
    "        Args:\n",
    "            pincode (str): Pincode of the city.\n",
    "            city (str): Name of the city.\n",
    "        \"\"\"\n",
    "        self.city = city\n",
    "        self.pincode = pincode\n",
    "        self.url='https://www.amazon.in/s?k=laptop'\n",
    "        self.link=f'{self.url}&pincode={self.pincode}'\n",
    "        self.HEADERS = {\n",
    "            # User-Agent and other headers to mimic a web browser request\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "            \"DNT\": \"1\",\n",
    "            \"Connection\": \"close\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"Accept-Language\": \"da, en-gb, en\",\n",
    "            \"referer\": \"https://prerender.io/\"\n",
    "        }  \n",
    "        \n",
    "        \n",
    "    def req_soup(self):\n",
    "        \"\"\"\n",
    "        Request and parse the HTML content of the webpage.\n",
    "\n",
    "        Returns:\n",
    "            bs4.element.ResultSet: A result set of links found on the webpage.\n",
    "        \"\"\"\n",
    "        webpage = requests.get(self.link,headers=self.HEADERS)\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "        Links = soup.find_all(\"a\",class_=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\")\n",
    "        return Links\n",
    "    \n",
    "    def get_product_name(self,soup):\n",
    "        \"\"\"\n",
    "        Extract the product name from the BeautifulSoup object.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object of the webpage.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted product name or an empty string if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            title = soup.find(\"span\",id=\"productTitle\").text.strip()\n",
    "            \n",
    "        except AttributeError:\n",
    "            title = \"\"\n",
    "                \n",
    "        return title\n",
    "    \n",
    "    def get_description(self,soup):\n",
    "        \"\"\"\n",
    "        Extract the description from the BeautifulSoup object.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object of the webpage.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted description or an empty string if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            description = soup.find(\"div\",id=\"feature-bullets\").text.strip()\n",
    "            \n",
    "        except AttributeError:\n",
    "            description = \"\"\n",
    "        \n",
    "        return description\n",
    "    \n",
    "    def get_category(self,soup):\n",
    "        \"\"\"\n",
    "        Extract the category from the BeautifulSoup object.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object of the webpage.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted category name or an empty string if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            category = soup.find(\"span\",class_=\"a-list-item\").text.strip()\n",
    "            \n",
    "        except AttributeError:\n",
    "            category = \"\"\n",
    "        \n",
    "        return category\n",
    "    \n",
    "    def get_mrp(self,soup):\n",
    "        \"\"\"\n",
    "        Extract the mrp price from the BeautifulSoup object.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object of the webpage.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted mrp price or an empty string if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            mrp = soup.find(\"span\",class_=\"a-price a-text-price\").find(\"span\",class_=\"a-offscreen\").text.strip()\n",
    "            \n",
    "        except AttributeError:\n",
    "            mrp = \"\"\n",
    "        \n",
    "        return mrp.replace('â‚¹','')\n",
    "    \n",
    "    def get_selling_price(self,soup):\n",
    "        \"\"\"\n",
    "        Extract the selling price from the BeautifulSoup object.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object of the webpage.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted selling price or an empty string if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            selling_price = soup.find(\"span\",class_=\"a-price-whole\").text\n",
    "            \n",
    "        except AttributeError:\n",
    "            selling_price = \"\"\n",
    "        \n",
    "        return selling_price\n",
    "    \n",
    "    def get_discount(self,soup):\n",
    "        \"\"\"\n",
    "        Extract the discount from the BeautifulSoup object.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object of the webpage.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted discount or an empty string if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            discount= soup.find(\"span\",class_=\"a-size-large a-color-price savingPriceOverride aok-align-center reinventPriceSavingsPercentageMargin savingsPercentage\").text\n",
    "            \n",
    "        except AttributeError:\n",
    "            discount = \"\"\n",
    "        \n",
    "        return discount.replace('-','')\n",
    "    \n",
    "    def get_brand_name(self,soup):\n",
    "        \"\"\"\n",
    "        Extract the brand name from the BeautifulSoup object.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object of the webpage.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted brand name or an empty string if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            brand_name= soup.find(\"span\",class_=\"a-size-base po-break-word\").text\n",
    "            \n",
    "        except AttributeError:\n",
    "            brand_name = \"\"\n",
    "        \n",
    "        return brand_name\n",
    "    \n",
    "    def get_image_url(self,soup):\n",
    "        \"\"\"\n",
    "        Extract the image address from the BeautifulSoup object.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object of the webpage.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted image address or an empty string if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            image_url= soup.find(\"div\",id=\"imgTagWrapperId\").find(\"img\").get(\"src\")\n",
    "            \n",
    "        except AttributeError:\n",
    "            image_url = \"\"\n",
    "        \n",
    "        return image_url\n",
    "    \n",
    "    def get_laptop_specification(self,soup):\n",
    "        \"\"\"\n",
    "        Extract the laptop specs from the BeautifulSoup object.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object of the webpage.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted laptop specs or an empty string if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            laptop_specification= soup.find(\"table\",class_=\"a-normal a-spacing-micro\").text.strip()\n",
    "            \n",
    "        except AttributeError:\n",
    "            laptop_specification = \"\"\n",
    "        \n",
    "        return laptop_specification\n",
    "    \n",
    "    def scrape(self):\n",
    "        \"\"\"\n",
    "        Scrape laptop data from Amazon and save it to a CSV file.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        Links_list=self.req_soup()\n",
    "        links_list=[]\n",
    "        for link in Links_list:\n",
    "            links_list.append(link.get('href'))\n",
    "\n",
    "        j=0\n",
    "\n",
    "        d={\"Product name\":[],\"Description\":[],\"Category\":[],\"MRP\":[],\"Selling price\":[],\"Discount\":[],\"Brand name\":[],\"Image url\":[],\"Laptop specification\":[]}\n",
    "        for link in links_list:\n",
    "            j+=1\n",
    "            if j>25:\n",
    "                break\n",
    "            new_webpage = requests.get(\"https://www.amazon.in\"+link,headers=self.HEADERS)\n",
    "            new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "            d['Product name'].append(self.get_product_name(new_soup))\n",
    "            d['Description'].append(self.get_description(new_soup))\n",
    "            d['Category'].append(self.get_category(new_soup))\n",
    "            d['MRP'].append(self.get_mrp(new_soup))\n",
    "            d['Selling price'].append(self.get_selling_price(new_soup))\n",
    "            d['Discount'].append(self.get_discount(new_soup))\n",
    "            d['Brand name'].append(self.get_brand_name(new_soup))\n",
    "            d['Image url'].append(self.get_image_url(new_soup))\n",
    "            d['Laptop specification'].append(self.get_laptop_specification(new_soup))\n",
    "            \n",
    "        \n",
    "        # Convert dictionary to DataFrame\n",
    "        amazon_data = pd.DataFrame.from_dict(d)\n",
    "        # Replace empty strings with NaN\n",
    "        amazon_data['Product name'].replace('', np.nan, inplace=True)\n",
    "        # Remove rows with NaN in 'Product name' column\n",
    "        amazon_data = amazon_data.dropna(subset=['Product name'])\n",
    "        # Save DataFrame to CSV file\n",
    "        amazon_data.to_csv(f\"laptop_list ({self.city}).csv\", header=True, index=False)\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f98cb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Instantiate and scrape data for Bangalore and Delhi\u001b[39;00m\n\u001b[0;32m      3\u001b[0m bangalore \u001b[38;5;241m=\u001b[39m AmazonLaptopScraper(pincode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m560001\u001b[39m\u001b[38;5;124m\"\u001b[39m, city\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbangalore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m bangalore\u001b[38;5;241m.\u001b[39mscrape()\n",
      "Cell \u001b[1;32mIn[4], line 235\u001b[0m, in \u001b[0;36mAmazonLaptopScraper.scrape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m     d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLaptop specification\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_laptop_specification(new_soup))\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Convert dictionary to JSON string\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m amazon_data_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(d)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Save JSON data to a gzip-compressed file\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlaptop_list_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# Instantiate and scrape data for Bangalore and Delhi\n",
    "\n",
    "bangalore = AmazonLaptopScraper(pincode=\"560001\", city=\"bangalore\")\n",
    "bangalore.scrape()\n",
    "# delhi = AmazonLaptopScraper(pincode=\"110001\", city=\"delhi\")\n",
    "# delhi.scrape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02d122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
